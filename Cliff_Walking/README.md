# 悬崖漫步（Cliff Walking）强化学习实验

## 项目简介

本项目实现了强化学习中的经典环境"悬崖漫步"（Cliff Walking），并使用多种强化学习算法求解最优策略，包括：
- 动态规划方法（策略迭代和价值迭代）
- 时序差分方法（Sarsa和Q-learning）

悬崖漫步是一个4×12的网格世界，智能体需要从起点（左下角）出发，避开悬崖区域，最终到达目标位置（右下角）。

## 环境需求

- JavaScript运行环境（Node.js）
- Python运行环境（用于时序差分算法）
- 所需Python包：numpy, matplotlib

## 运行方法

### 动态规划算法

直接运行 `cliff_walking.js` 文件：

```bash
node cliff_walking.js
```

程序会执行两种动态规划算法（策略迭代和价值迭代），并分别输出它们的结果。

### 时序差分算法

运行 `temporal_difference_algorithms.py` 文件：

```bash
python temporal_difference_algorithms.py
```

程序会执行Sarsa和Q-learning算法，并生成学习曲线对比图。

## 可视化运行

打开 `cliff_walking_visualization.html` 文件可以在浏览器中查看环境的可视化界面：

```bash
open cliff_walking_visualization.html
```

可视化界面功能：
- 显示悬崖漫步环境的初始状态
- 点击"运行策略迭代"按钮执行策略迭代算法
- 点击"运行价值迭代"按钮执行价值迭代算法
- 实时显示每个状态的价值和最优动作

### 可视化界面预览

![悬崖漫步可视化界面](cliff_walking.jpg)

### 时序差分算法可视化

运行 `temporal_difference_visualization.html` 可以在浏览器中查看Sarsa和Q-learning算法的可视化对比界面：

```bash
open temporal_difference_visualization.html
```

时序差分可视化界面功能：
- 同时显示Sarsa和Q-learning两种算法的训练结果
- 支持单步训练和批量训练
- 实时显示每个算法的策略和状态价值
- 提供全局控制按钮同时操作两个算法
- 显示训练统计信息（回合数、奖励等）

## 代码结构

### 动态规划算法（JavaScript）

1. `CliffWalkingEnv`：实现悬崖漫步环境，定义状态转移和奖励函数
2. `PolicyIteration`：实现策略迭代算法
3. `ValueIteration`：实现价值迭代算法
4. 辅助函数 `printAgent`：用于可视化输出状态价值和最优策略

### 时序差分算法（Python）

1. `CliffWalkingEnv`：悬崖漫步环境（Python版本）
2. `Sarsa`：Sarsa算法实现（在线策略）
3. `QLearning`：Q-learning算法实现（离线策略）
4. 训练和评估函数

## 结果解读

运行程序后，会看到不同算法的结果输出：

### 1. 状态价值函数 V(s)

以表格形式展示每个状态的价值，数值越高表示该状态越有价值。

### 2. 最优策略 π(s)

以符号形式展示每个状态下的最优动作：
- `↑`：向上移动
- `↓`：向下移动
- `←`：向左移动
- `→`：向右移动
- `C`：悬崖位置
- `G`：目标位置

## 算法比较

### 动态规划 vs 时序差分

- **动态规划**：需要完全已知环境模型（状态转移概率和奖励函数）
- **时序差分**：通过与环境交互学习，不需要事先知道环境模型

### Sarsa vs Q-learning

- **Sarsa（在线策略）**：学习当前执行策略的价值，较为保守
- **Q-learning（离线策略）**：学习最优策略的价值，较为激进

在悬崖漫步环境中：
- Sarsa学到的策略会远离悬崖边缘，训练过程中获得更高回报
- Q-learning学到的策略沿着悬崖边缘行走，理论上是最优的但训练过程风险更高

## 参数调整

如需调整算法参数，可修改代码中的以下值：
- `theta`：算法收敛阈值（默认1e-5）
- `gamma`：折扣因子（默认0.9）
- `epsilon`：ε-贪婪策略参数（默认0.1）
- `alpha`：学习率（默认0.1）

## 悬崖漫步环境说明

- 每走一步奖励为-1
- 掉入悬崖奖励为-100
- 到达目标会结束当前回合
- 掉入悬崖会结束当前回合并回到起点 

## 算法实现

本项目实现了四种强化学习算法：

### 动态规划算法

1. **策略迭代算法**：先进行策略评估（使用贝尔曼期望方程），再进行策略提升，不断迭代直到策略稳定。

2. **价值迭代算法**：直接使用贝尔曼最优方程来进行动态规划，得到最优状态价值，然后提取最优策略。

### 时序差分算法

3. **Sarsa算法**：在线策略时序差分算法，使用当前策略选择的动作进行更新。

4. **Q-learning算法**：离线策略时序差分算法，使用贪婪策略选择的动作进行更新。

算法实现逻辑参考了["动手学强化学习"教程](https://hrl.boyuai.com/chapter/1/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95)。

## 更新日志

### 2023-xx-xx
- 优化UI界面，改进显示效果
- 将策略迭代结果显示格式调整为单行显示，与价值迭代结果格式保持一致
- 修复价值迭代单步运行时策略表格不更新的问题，现在每次价值更新后都会同步更新策略
- 新增时序差分算法实现：Sarsa和Q-learning算法
- 添加算法性能对比分析和可视化图表
- 修复matplotlib中文字体显示问题，将图表标签改为英文以提高兼容性
- 创建时序差分算法可视化界面，支持Sarsa和Q-learning算法的实时对比展示
- 实现单步训练、批量训练和重置功能，便于观察算法学习过程 