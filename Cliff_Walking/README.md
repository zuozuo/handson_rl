# 悬崖漫步（Cliff Walking）强化学习实验

## 项目简介

本项目实现了强化学习中的经典环境"悬崖漫步"（Cliff Walking），并使用动态规划方法（策略迭代和价值迭代）求解最优策略。悬崖漫步是一个4×12的网格世界，智能体需要从起点（左下角）出发，避开悬崖区域，最终到达目标位置（右下角）。

## 环境需求

- Python 3.6+
- NumPy

## 安装依赖

```bash
pip install numpy
```

## 运行方法

直接运行 `cliff_walking_dp.py` 文件：

```bash
python cliff_walking_dp.py
```

程序会执行两种动态规划算法（策略迭代和价值迭代），并分别输出它们的结果。

## 代码结构

代码主要包含以下几个类：

1. `CliffWalkingEnv`：实现悬崖漫步环境，定义状态转移和奖励函数
2. `PolicyIteration`：实现策略迭代算法
3. `ValueIteration`：实现价值迭代算法
4. 辅助函数 `print_agent`：用于可视化输出状态价值和最优策略

## 结果解读

运行程序后，会看到两种算法的结果输出：

### 1. 状态价值函数 V(s)

以表格形式展示每个状态的价值，数值越高表示该状态越有价值。

### 2. 最优策略 π(s)

以符号形式展示每个状态下的最优动作：
- `^`：向上移动
- `v`：向下移动
- `<`：向左移动
- `>`：向右移动
- `C`：悬崖位置
- `G`：目标位置

## 参数调整

如需调整算法参数，可修改代码中的以下值：
- `theta`：算法收敛阈值（默认1e-5）
- `gamma`：折扣因子（默认0.9）

## 悬崖漫步环境说明

- 每走一步奖励为-1
- 掉入悬崖奖励为-100
- 到达目标会结束当前回合
- 掉入悬崖会结束当前回合并回到起点 